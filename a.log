>> TRAIN ALL (word2vec) <<
> training <main> ...
Using TensorFlow backend.
layer_1_cnn_word2vec.py:122: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_1_cnn_word2vec.py:123: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_1_cnn_word2vec.py:124: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_1_cnn_word2vec.py:125: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_1_cnn_word2vec.py:133: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_1_cnn_word2vec.py:141: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_1_cnn_word2vec.py:145: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs)) #validation_data = (x_test, y_test))
2018-09-24 22:03:36.473310: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:03:36.473347: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:03:36.473354: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:03:36.473360: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:03:36.473366: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:03:36.594192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-24 22:03:36.594678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2018-09-24 22:03:36.594696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-09-24 22:03:36.594704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-09-24 22:03:36.594714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
(86, 9600) (86,)
(1162, 9600) (1162,)
(1250, 9600) (1250,)
(1223, 9600) (1223,)
(835, 9600) (835,)
(896, 9600) (896,)
(9, 9600) (9,)
(138, 9600) (138,)
(94, 9600) (94,)
(65, 9600) (65,)
(81, 9600) (81,)
(113, 9600) (113,)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 32, 300, 1)    0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 31, 1, 500)    300500      input_1[0][0]                    
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 30, 1, 500)    450500      input_1[0][0]                    
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 29, 1, 500)    600500      input_1[0][0]                    
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 1, 1, 500)     0           conv2d_1[0][0]                   
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 1, 1, 500)     0           conv2d_2[0][0]                   
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 1, 1, 500)     0           conv2d_3[0][0]                   
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 1, 1, 1500)    0           max_pooling2d_1[0][0]            
                                                                   max_pooling2d_2[0][0]            
                                                                   max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 1500)          0           merge_1[0][0]                    
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           192128      flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            8256        dropout_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 64)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 6)             390         dropout_2[0][0]                  
====================================================================================================
Total params: 1,552,274
Trainable params: 1,552,274
Non-trainable params: 0
____________________________________________________________________________________________________
Epoch 1/10
  50/5452 [..............................] - ETA: 101s - loss: 1.7897 - acc: 0.1000 200/5452 [>.............................] - ETA: 26s - loss: 1.7191 - acc: 0.2200  350/5452 [>.............................] - ETA: 15s - loss: 1.6527 - acc: 0.2886 500/5452 [=>............................] - ETA: 11s - loss: 1.5849 - acc: 0.3420 650/5452 [==>...........................] - ETA: 8s - loss: 1.5324 - acc: 0.3754  800/5452 [===>..........................] - ETA: 7s - loss: 1.4726 - acc: 0.4112 950/5452 [====>.........................] - ETA: 6s - loss: 1.4250 - acc: 0.43471100/5452 [=====>........................] - ETA: 5s - loss: 1.3573 - acc: 0.46451250/5452 [=====>........................] - ETA: 4s - loss: 1.3108 - acc: 0.48561400/5452 [======>.......................] - ETA: 4s - loss: 1.2751 - acc: 0.50291550/5452 [=======>......................] - ETA: 3s - loss: 1.2348 - acc: 0.52061700/5452 [========>.....................] - ETA: 3s - loss: 1.2052 - acc: 0.53351850/5452 [=========>....................] - ETA: 3s - loss: 1.1707 - acc: 0.54762000/5452 [==========>...................] - ETA: 2s - loss: 1.1326 - acc: 0.56452150/5452 [==========>...................] - ETA: 2s - loss: 1.1110 - acc: 0.57352300/5452 [===========>..................] - ETA: 2s - loss: 1.0886 - acc: 0.58392450/5452 [============>.................] - ETA: 2s - loss: 1.0607 - acc: 0.59552650/5452 [=============>................] - ETA: 2s - loss: 1.0255 - acc: 0.61132800/5452 [==============>...............] - ETA: 1s - loss: 1.0043 - acc: 0.61962950/5452 [===============>..............] - ETA: 1s - loss: 0.9837 - acc: 0.62813100/5452 [================>.............] - ETA: 1s - loss: 0.9607 - acc: 0.63773250/5452 [================>.............] - ETA: 1s - loss: 0.9422 - acc: 0.64683400/5452 [=================>............] - ETA: 1s - loss: 0.9278 - acc: 0.65213550/5452 [==================>...........] - ETA: 1s - loss: 0.9100 - acc: 0.66033700/5452 [===================>..........] - ETA: 1s - loss: 0.8936 - acc: 0.66653900/5452 [====================>.........] - ETA: 0s - loss: 0.8805 - acc: 0.67264100/5452 [=====================>........] - ETA: 0s - loss: 0.8653 - acc: 0.67884300/5452 [======================>.......] - ETA: 0s - loss: 0.8481 - acc: 0.68654500/5452 [=======================>......] - ETA: 0s - loss: 0.8326 - acc: 0.69274650/5452 [========================>.....] - ETA: 0s - loss: 0.8231 - acc: 0.69664850/5452 [=========================>....] - ETA: 0s - loss: 0.8165 - acc: 0.70045050/5452 [==========================>...] - ETA: 0s - loss: 0.8006 - acc: 0.70735200/5452 [===========================>..] - ETA: 0s - loss: 0.7896 - acc: 0.71175400/5452 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.71635452/5452 [==============================] - 2s - loss: 0.7717 - acc: 0.7179     
Epoch 2/10
  50/5452 [..............................] - ETA: 1s - loss: 0.3752 - acc: 0.8600 250/5452 [>.............................] - ETA: 1s - loss: 0.3354 - acc: 0.8920 400/5452 [=>............................] - ETA: 1s - loss: 0.3410 - acc: 0.8875 600/5452 [==>...........................] - ETA: 1s - loss: 0.3567 - acc: 0.8883 800/5452 [===>..........................] - ETA: 1s - loss: 0.3522 - acc: 0.8925 950/5452 [====>.........................] - ETA: 1s - loss: 0.3543 - acc: 0.89371100/5452 [=====>........................] - ETA: 1s - loss: 0.3552 - acc: 0.89181250/5452 [=====>........................] - ETA: 1s - loss: 0.3511 - acc: 0.89201400/5452 [======>.......................] - ETA: 1s - loss: 0.3440 - acc: 0.89211550/5452 [=======>......................] - ETA: 1s - loss: 0.3351 - acc: 0.89351750/5452 [========>.....................] - ETA: 1s - loss: 0.3322 - acc: 0.89601950/5452 [=========>....................] - ETA: 1s - loss: 0.3237 - acc: 0.89902150/5452 [==========>...................] - ETA: 1s - loss: 0.3162 - acc: 0.90282350/5452 [===========>..................] - ETA: 1s - loss: 0.3174 - acc: 0.90172550/5452 [=============>................] - ETA: 0s - loss: 0.3080 - acc: 0.90552700/5452 [=============>................] - ETA: 0s - loss: 0.3071 - acc: 0.90482850/5452 [==============>...............] - ETA: 0s - loss: 0.3066 - acc: 0.90493050/5452 [===============>..............] - ETA: 0s - loss: 0.3088 - acc: 0.90433250/5452 [================>.............] - ETA: 0s - loss: 0.3048 - acc: 0.90463450/5452 [=================>............] - ETA: 0s - loss: 0.3025 - acc: 0.90493650/5452 [===================>..........] - ETA: 0s - loss: 0.3033 - acc: 0.90523850/5452 [====================>.........] - ETA: 0s - loss: 0.3029 - acc: 0.90474050/5452 [=====================>........] - ETA: 0s - loss: 0.3060 - acc: 0.90204200/5452 [======================>.......] - ETA: 0s - loss: 0.3063 - acc: 0.90124400/5452 [=======================>......] - ETA: 0s - loss: 0.3077 - acc: 0.90004600/5452 [========================>.....] - ETA: 0s - loss: 0.3040 - acc: 0.90154800/5452 [=========================>....] - ETA: 0s - loss: 0.3034 - acc: 0.90154950/5452 [==========================>...] - ETA: 0s - loss: 0.3021 - acc: 0.90125150/5452 [===========================>..] - ETA: 0s - loss: 0.2981 - acc: 0.90295350/5452 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.90325452/5452 [==============================] - 1s - loss: 0.2983 - acc: 0.9033     
Epoch 3/10
  50/5452 [..............................] - ETA: 1s - loss: 0.1084 - acc: 1.0000 250/5452 [>.............................] - ETA: 1s - loss: 0.1544 - acc: 0.9600 450/5452 [=>............................] - ETA: 1s - loss: 0.1614 - acc: 0.9622 650/5452 [==>...........................] - ETA: 1s - loss: 0.1649 - acc: 0.9600 850/5452 [===>..........................] - ETA: 1s - loss: 0.1555 - acc: 0.95881050/5452 [====>.........................] - ETA: 1s - loss: 0.1469 - acc: 0.96191250/5452 [=====>........................] - ETA: 1s - loss: 0.1383 - acc: 0.96321450/5452 [======>.......................] - ETA: 1s - loss: 0.1320 - acc: 0.96621650/5452 [========>.....................] - ETA: 1s - loss: 0.1324 - acc: 0.96551850/5452 [=========>....................] - ETA: 1s - loss: 0.1322 - acc: 0.96542050/5452 [==========>...................] - ETA: 1s - loss: 0.1322 - acc: 0.96442250/5452 [===========>..................] - ETA: 1s - loss: 0.1337 - acc: 0.96272450/5452 [============>.................] - ETA: 0s - loss: 0.1346 - acc: 0.96122650/5452 [=============>................] - ETA: 0s - loss: 0.1324 - acc: 0.96152850/5452 [==============>...............] - ETA: 0s - loss: 0.1295 - acc: 0.96213050/5452 [===============>..............] - ETA: 0s - loss: 0.1309 - acc: 0.96163250/5452 [================>.............] - ETA: 0s - loss: 0.1305 - acc: 0.96223450/5452 [=================>............] - ETA: 0s - loss: 0.1296 - acc: 0.96263650/5452 [===================>..........] - ETA: 0s - loss: 0.1308 - acc: 0.96163850/5452 [====================>.........] - ETA: 0s - loss: 0.1289 - acc: 0.96214050/5452 [=====================>........] - ETA: 0s - loss: 0.1274 - acc: 0.96254250/5452 [======================>.......] - ETA: 0s - loss: 0.1276 - acc: 0.96264450/5452 [=======================>......] - ETA: 0s - loss: 0.1269 - acc: 0.96254600/5452 [========================>.....] - ETA: 0s - loss: 0.1265 - acc: 0.96264750/5452 [=========================>....] - ETA: 0s - loss: 0.1286 - acc: 0.96174950/5452 [==========================>...] - ETA: 0s - loss: 0.1272 - acc: 0.96185150/5452 [===========================>..] - ETA: 0s - loss: 0.1299 - acc: 0.96175350/5452 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.96155452/5452 [==============================] - 1s - loss: 0.1310 - acc: 0.9615     
Epoch 4/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0778 - acc: 1.0000 200/5452 [>.............................] - ETA: 1s - loss: 0.0646 - acc: 0.9850 400/5452 [=>............................] - ETA: 1s - loss: 0.0723 - acc: 0.9775 600/5452 [==>...........................] - ETA: 1s - loss: 0.0753 - acc: 0.9767 750/5452 [===>..........................] - ETA: 1s - loss: 0.0766 - acc: 0.9773 950/5452 [====>.........................] - ETA: 1s - loss: 0.0713 - acc: 0.98001150/5452 [=====>........................] - ETA: 1s - loss: 0.0747 - acc: 0.98091300/5452 [======>.......................] - ETA: 1s - loss: 0.0736 - acc: 0.98081500/5452 [=======>......................] - ETA: 1s - loss: 0.0698 - acc: 0.98271700/5452 [========>.....................] - ETA: 1s - loss: 0.0657 - acc: 0.98291900/5452 [=========>....................] - ETA: 1s - loss: 0.0618 - acc: 0.98422100/5452 [==========>...................] - ETA: 1s - loss: 0.0647 - acc: 0.98192250/5452 [===========>..................] - ETA: 1s - loss: 0.0670 - acc: 0.98092450/5452 [============>.................] - ETA: 0s - loss: 0.0659 - acc: 0.98042650/5452 [=============>................] - ETA: 0s - loss: 0.0646 - acc: 0.98112850/5452 [==============>...............] - ETA: 0s - loss: 0.0630 - acc: 0.98253050/5452 [===============>..............] - ETA: 0s - loss: 0.0612 - acc: 0.98333250/5452 [================>.............] - ETA: 0s - loss: 0.0620 - acc: 0.98283450/5452 [=================>............] - ETA: 0s - loss: 0.0608 - acc: 0.98323600/5452 [==================>...........] - ETA: 0s - loss: 0.0603 - acc: 0.98333800/5452 [===================>..........] - ETA: 0s - loss: 0.0591 - acc: 0.98394000/5452 [=====================>........] - ETA: 0s - loss: 0.0600 - acc: 0.98304200/5452 [======================>.......] - ETA: 0s - loss: 0.0594 - acc: 0.98314400/5452 [=======================>......] - ETA: 0s - loss: 0.0587 - acc: 0.98324600/5452 [========================>.....] - ETA: 0s - loss: 0.0598 - acc: 0.98224800/5452 [=========================>....] - ETA: 0s - loss: 0.0613 - acc: 0.98124950/5452 [==========================>...] - ETA: 0s - loss: 0.0606 - acc: 0.98165100/5452 [===========================>..] - ETA: 0s - loss: 0.0610 - acc: 0.98185250/5452 [===========================>..] - ETA: 0s - loss: 0.0615 - acc: 0.98155450/5452 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.98185452/5452 [==============================] - 1s - loss: 0.0603 - acc: 0.9818     
Epoch 5/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0437 - acc: 0.9800 200/5452 [>.............................] - ETA: 1s - loss: 0.0743 - acc: 0.9700 400/5452 [=>............................] - ETA: 1s - loss: 0.0512 - acc: 0.9825 600/5452 [==>...........................] - ETA: 1s - loss: 0.0473 - acc: 0.9833 750/5452 [===>..........................] - ETA: 1s - loss: 0.0450 - acc: 0.9867 950/5452 [====>.........................] - ETA: 1s - loss: 0.0408 - acc: 0.98841150/5452 [=====>........................] - ETA: 1s - loss: 0.0399 - acc: 0.98871350/5452 [======>.......................] - ETA: 1s - loss: 0.0372 - acc: 0.99041550/5452 [=======>......................] - ETA: 1s - loss: 0.0396 - acc: 0.99101750/5452 [========>.....................] - ETA: 1s - loss: 0.0382 - acc: 0.99141950/5452 [=========>....................] - ETA: 1s - loss: 0.0387 - acc: 0.99132150/5452 [==========>...................] - ETA: 1s - loss: 0.0381 - acc: 0.99072350/5452 [===========>..................] - ETA: 1s - loss: 0.0382 - acc: 0.99112550/5452 [=============>................] - ETA: 0s - loss: 0.0394 - acc: 0.99062750/5452 [==============>...............] - ETA: 0s - loss: 0.0432 - acc: 0.98912950/5452 [===============>..............] - ETA: 0s - loss: 0.0419 - acc: 0.98953150/5452 [================>.............] - ETA: 0s - loss: 0.0418 - acc: 0.98923350/5452 [=================>............] - ETA: 0s - loss: 0.0403 - acc: 0.98993550/5452 [==================>...........] - ETA: 0s - loss: 0.0398 - acc: 0.98993750/5452 [===================>..........] - ETA: 0s - loss: 0.0389 - acc: 0.99043950/5452 [====================>.........] - ETA: 0s - loss: 0.0384 - acc: 0.99044150/5452 [=====================>........] - ETA: 0s - loss: 0.0389 - acc: 0.99014350/5452 [======================>.......] - ETA: 0s - loss: 0.0385 - acc: 0.98994550/5452 [========================>.....] - ETA: 0s - loss: 0.0380 - acc: 0.98974700/5452 [========================>.....] - ETA: 0s - loss: 0.0374 - acc: 0.98984900/5452 [=========================>....] - ETA: 0s - loss: 0.0388 - acc: 0.98985050/5452 [==========================>...] - ETA: 0s - loss: 0.0383 - acc: 0.98995250/5452 [===========================>..] - ETA: 0s - loss: 0.0408 - acc: 0.98935450/5452 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.98925452/5452 [==============================] - 1s - loss: 0.0410 - acc: 0.9892     
Epoch 6/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0084 - acc: 1.0000 250/5452 [>.............................] - ETA: 1s - loss: 0.0304 - acc: 0.9960 450/5452 [=>............................] - ETA: 1s - loss: 0.0241 - acc: 0.9956 650/5452 [==>...........................] - ETA: 1s - loss: 0.0198 - acc: 0.9969 800/5452 [===>..........................] - ETA: 1s - loss: 0.0299 - acc: 0.99501000/5452 [====>.........................] - ETA: 1s - loss: 0.0287 - acc: 0.99501200/5452 [=====>........................] - ETA: 1s - loss: 0.0269 - acc: 0.99501350/5452 [======>.......................] - ETA: 1s - loss: 0.0264 - acc: 0.99481550/5452 [=======>......................] - ETA: 1s - loss: 0.0275 - acc: 0.99421750/5452 [========>.....................] - ETA: 1s - loss: 0.0270 - acc: 0.99491950/5452 [=========>....................] - ETA: 1s - loss: 0.0282 - acc: 0.99332100/5452 [==========>...................] - ETA: 1s - loss: 0.0296 - acc: 0.99332300/5452 [===========>..................] - ETA: 1s - loss: 0.0297 - acc: 0.99302500/5452 [============>.................] - ETA: 0s - loss: 0.0285 - acc: 0.99362650/5452 [=============>................] - ETA: 0s - loss: 0.0300 - acc: 0.99282850/5452 [==============>...............] - ETA: 0s - loss: 0.0297 - acc: 0.99263050/5452 [===============>..............] - ETA: 0s - loss: 0.0289 - acc: 0.99283250/5452 [================>.............] - ETA: 0s - loss: 0.0278 - acc: 0.99323450/5452 [=================>............] - ETA: 0s - loss: 0.0270 - acc: 0.99333650/5452 [===================>..........] - ETA: 0s - loss: 0.0278 - acc: 0.99263850/5452 [====================>.........] - ETA: 0s - loss: 0.0275 - acc: 0.99274050/5452 [=====================>........] - ETA: 0s - loss: 0.0277 - acc: 0.99264250/5452 [======================>.......] - ETA: 0s - loss: 0.0273 - acc: 0.99274450/5452 [=======================>......] - ETA: 0s - loss: 0.0278 - acc: 0.99264650/5452 [========================>.....] - ETA: 0s - loss: 0.0275 - acc: 0.99274850/5452 [=========================>....] - ETA: 0s - loss: 0.0275 - acc: 0.99285050/5452 [==========================>...] - ETA: 0s - loss: 0.0272 - acc: 0.99295250/5452 [===========================>..] - ETA: 0s - loss: 0.0279 - acc: 0.99245450/5452 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.99235452/5452 [==============================] - 1s - loss: 0.0277 - acc: 0.9923     
Epoch 7/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0050 - acc: 1.0000 250/5452 [>.............................] - ETA: 1s - loss: 0.0111 - acc: 1.0000 400/5452 [=>............................] - ETA: 1s - loss: 0.0120 - acc: 1.0000 600/5452 [==>...........................] - ETA: 1s - loss: 0.0179 - acc: 0.9983 750/5452 [===>..........................] - ETA: 1s - loss: 0.0335 - acc: 0.9920 950/5452 [====>.........................] - ETA: 1s - loss: 0.0324 - acc: 0.99261150/5452 [=====>........................] - ETA: 1s - loss: 0.0282 - acc: 0.99391350/5452 [======>.......................] - ETA: 1s - loss: 0.0256 - acc: 0.99481550/5452 [=======>......................] - ETA: 1s - loss: 0.0245 - acc: 0.99481750/5452 [========>.....................] - ETA: 1s - loss: 0.0274 - acc: 0.99371950/5452 [=========>....................] - ETA: 1s - loss: 0.0257 - acc: 0.99382150/5452 [==========>...................] - ETA: 1s - loss: 0.0241 - acc: 0.99442350/5452 [===========>..................] - ETA: 1s - loss: 0.0234 - acc: 0.99452550/5452 [=============>................] - ETA: 0s - loss: 0.0221 - acc: 0.99492750/5452 [==============>...............] - ETA: 0s - loss: 0.0211 - acc: 0.99532950/5452 [===============>..............] - ETA: 0s - loss: 0.0213 - acc: 0.99533150/5452 [================>.............] - ETA: 0s - loss: 0.0205 - acc: 0.99563300/5452 [=================>............] - ETA: 0s - loss: 0.0205 - acc: 0.99553500/5452 [==================>...........] - ETA: 0s - loss: 0.0200 - acc: 0.99543700/5452 [===================>..........] - ETA: 0s - loss: 0.0194 - acc: 0.99573900/5452 [====================>.........] - ETA: 0s - loss: 0.0192 - acc: 0.99564100/5452 [=====================>........] - ETA: 0s - loss: 0.0187 - acc: 0.99594250/5452 [======================>.......] - ETA: 0s - loss: 0.0183 - acc: 0.99604400/5452 [=======================>......] - ETA: 0s - loss: 0.0196 - acc: 0.99594600/5452 [========================>.....] - ETA: 0s - loss: 0.0192 - acc: 0.99614800/5452 [=========================>....] - ETA: 0s - loss: 0.0201 - acc: 0.99605000/5452 [==========================>...] - ETA: 0s - loss: 0.0196 - acc: 0.99625200/5452 [===========================>..] - ETA: 0s - loss: 0.0196 - acc: 0.99625400/5452 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.99635452/5452 [==============================] - 1s - loss: 0.0191 - acc: 0.9963     
Epoch 8/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0205 - acc: 1.0000 250/5452 [>.............................] - ETA: 1s - loss: 0.0374 - acc: 0.9920 400/5452 [=>............................] - ETA: 1s - loss: 0.0244 - acc: 0.9950 600/5452 [==>...........................] - ETA: 1s - loss: 0.0182 - acc: 0.9967 800/5452 [===>..........................] - ETA: 1s - loss: 0.0155 - acc: 0.99751000/5452 [====>.........................] - ETA: 1s - loss: 0.0134 - acc: 0.99801200/5452 [=====>........................] - ETA: 1s - loss: 0.0140 - acc: 0.99751400/5452 [======>.......................] - ETA: 1s - loss: 0.0130 - acc: 0.99791600/5452 [=======>......................] - ETA: 1s - loss: 0.0127 - acc: 0.99751800/5452 [========>.....................] - ETA: 1s - loss: 0.0118 - acc: 0.99781950/5452 [=========>....................] - ETA: 1s - loss: 0.0116 - acc: 0.99792150/5452 [==========>...................] - ETA: 1s - loss: 0.0110 - acc: 0.99812350/5452 [===========>..................] - ETA: 1s - loss: 0.0108 - acc: 0.99832550/5452 [=============>................] - ETA: 0s - loss: 0.0122 - acc: 0.99762750/5452 [==============>...............] - ETA: 0s - loss: 0.0131 - acc: 0.99752950/5452 [===============>..............] - ETA: 0s - loss: 0.0128 - acc: 0.99763150/5452 [================>.............] - ETA: 0s - loss: 0.0124 - acc: 0.99783350/5452 [=================>............] - ETA: 0s - loss: 0.0129 - acc: 0.99763550/5452 [==================>...........] - ETA: 0s - loss: 0.0127 - acc: 0.99753750/5452 [===================>..........] - ETA: 0s - loss: 0.0124 - acc: 0.99763950/5452 [====================>.........] - ETA: 0s - loss: 0.0131 - acc: 0.99754150/5452 [=====================>........] - ETA: 0s - loss: 0.0146 - acc: 0.99694350/5452 [======================>.......] - ETA: 0s - loss: 0.0143 - acc: 0.99704550/5452 [========================>.....] - ETA: 0s - loss: 0.0156 - acc: 0.99654750/5452 [=========================>....] - ETA: 0s - loss: 0.0161 - acc: 0.99624950/5452 [==========================>...] - ETA: 0s - loss: 0.0160 - acc: 0.99645150/5452 [===========================>..] - ETA: 0s - loss: 0.0174 - acc: 0.99615350/5452 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.99635452/5452 [==============================] - 1s - loss: 0.0173 - acc: 0.9960     
Epoch 9/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0069 - acc: 1.0000 250/5452 [>.............................] - ETA: 1s - loss: 0.0268 - acc: 0.9920 450/5452 [=>............................] - ETA: 1s - loss: 0.0244 - acc: 0.9933 650/5452 [==>...........................] - ETA: 1s - loss: 0.0181 - acc: 0.9954 850/5452 [===>..........................] - ETA: 1s - loss: 0.0236 - acc: 0.99531000/5452 [====>.........................] - ETA: 1s - loss: 0.0229 - acc: 0.99501200/5452 [=====>........................] - ETA: 1s - loss: 0.0200 - acc: 0.99581400/5452 [======>.......................] - ETA: 1s - loss: 0.0208 - acc: 0.99501550/5452 [=======>......................] - ETA: 1s - loss: 0.0228 - acc: 0.99421750/5452 [========>.....................] - ETA: 1s - loss: 0.0211 - acc: 0.99491900/5452 [=========>....................] - ETA: 1s - loss: 0.0200 - acc: 0.99532100/5452 [==========>...................] - ETA: 1s - loss: 0.0192 - acc: 0.99522300/5452 [===========>..................] - ETA: 1s - loss: 0.0180 - acc: 0.99572500/5452 [============>.................] - ETA: 0s - loss: 0.0171 - acc: 0.99602700/5452 [=============>................] - ETA: 0s - loss: 0.0169 - acc: 0.99592900/5452 [==============>...............] - ETA: 0s - loss: 0.0163 - acc: 0.99623100/5452 [================>.............] - ETA: 0s - loss: 0.0155 - acc: 0.99653250/5452 [================>.............] - ETA: 0s - loss: 0.0150 - acc: 0.99663450/5452 [=================>............] - ETA: 0s - loss: 0.0145 - acc: 0.99683600/5452 [==================>...........] - ETA: 0s - loss: 0.0149 - acc: 0.99673800/5452 [===================>..........] - ETA: 0s - loss: 0.0147 - acc: 0.99634000/5452 [=====================>........] - ETA: 0s - loss: 0.0142 - acc: 0.99654200/5452 [======================>.......] - ETA: 0s - loss: 0.0137 - acc: 0.99674350/5452 [======================>.......] - ETA: 0s - loss: 0.0139 - acc: 0.99664500/5452 [=======================>......] - ETA: 0s - loss: 0.0135 - acc: 0.99674650/5452 [========================>.....] - ETA: 0s - loss: 0.0135 - acc: 0.99664850/5452 [=========================>....] - ETA: 0s - loss: 0.0136 - acc: 0.99655050/5452 [==========================>...] - ETA: 0s - loss: 0.0134 - acc: 0.99665250/5452 [===========================>..] - ETA: 0s - loss: 0.0132 - acc: 0.99685450/5452 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.99675452/5452 [==============================] - 1s - loss: 0.0151 - acc: 0.9967     
Epoch 10/10
  50/5452 [..............................] - ETA: 1s - loss: 0.0432 - acc: 0.9800 250/5452 [>.............................] - ETA: 1s - loss: 0.0192 - acc: 0.9960 400/5452 [=>............................] - ETA: 1s - loss: 0.0161 - acc: 0.9950 600/5452 [==>...........................] - ETA: 1s - loss: 0.0142 - acc: 0.9950 800/5452 [===>..........................] - ETA: 1s - loss: 0.0121 - acc: 0.99621000/5452 [====>.........................] - ETA: 1s - loss: 0.0107 - acc: 0.99701200/5452 [=====>........................] - ETA: 1s - loss: 0.0095 - acc: 0.99751400/5452 [======>.......................] - ETA: 1s - loss: 0.0088 - acc: 0.99791600/5452 [=======>......................] - ETA: 1s - loss: 0.0081 - acc: 0.99811800/5452 [========>.....................] - ETA: 1s - loss: 0.0106 - acc: 0.99782000/5452 [==========>...................] - ETA: 1s - loss: 0.0106 - acc: 0.99752200/5452 [===========>..................] - ETA: 1s - loss: 0.0111 - acc: 0.99732400/5452 [============>.................] - ETA: 0s - loss: 0.0109 - acc: 0.99752600/5452 [=============>................] - ETA: 0s - loss: 0.0129 - acc: 0.99732800/5452 [==============>...............] - ETA: 0s - loss: 0.0124 - acc: 0.99753000/5452 [===============>..............] - ETA: 0s - loss: 0.0119 - acc: 0.99773200/5452 [================>.............] - ETA: 0s - loss: 0.0113 - acc: 0.99783400/5452 [=================>............] - ETA: 0s - loss: 0.0126 - acc: 0.99743600/5452 [==================>...........] - ETA: 0s - loss: 0.0123 - acc: 0.99753800/5452 [===================>..........] - ETA: 0s - loss: 0.0119 - acc: 0.99764000/5452 [=====================>........] - ETA: 0s - loss: 0.0126 - acc: 0.99734200/5452 [======================>.......] - ETA: 0s - loss: 0.0122 - acc: 0.99744400/5452 [=======================>......] - ETA: 0s - loss: 0.0118 - acc: 0.99754600/5452 [========================>.....] - ETA: 0s - loss: 0.0132 - acc: 0.99744800/5452 [=========================>....] - ETA: 0s - loss: 0.0137 - acc: 0.99714950/5452 [==========================>...] - ETA: 0s - loss: 0.0141 - acc: 0.99705100/5452 [===========================>..] - ETA: 0s - loss: 0.0185 - acc: 0.99635300/5452 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.99625452/5452 [==============================] - 1s - loss: 0.0216 - acc: 0.9952     
0.884
[[  8   1   0   0   0   0]
 [  1 118  16   0   1   2]
 [  0   6  81   0   6   1]
 [  0   0   7  54   4   0]
 [  0   0   2   0  79   0]
 [  0   1   7   0   3 102]]
> training sub-category <abbreviatio>n ..
Using TensorFlow backend.
layer_2_cnn_word2vec.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:137: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3, max_pool_4], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_2_cnn_word2vec.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_2_cnn_word2vec.py:148: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
2018-09-24 22:04:46.602585: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:04:46.602621: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:04:46.602628: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:04:46.602633: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:04:46.602638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:04:46.722127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-24 22:04:46.722610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2018-09-24 22:04:46.722628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-09-24 22:04:46.722635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-09-24 22:04:46.722645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
(70, 9600) (70,)
(16, 9600) (16,)
(8, 9600) (8,)
(1, 9600) (1,)
(9, 9600)
Train on 86 samples, validate on 9 samples
Epoch 1/10
50/86 [================>.............] - ETA: 0s - loss: 0.6879 - acc: 0.560086/86 [==============================] - 1s - loss: 0.5837 - acc: 0.6744 - val_loss: 0.3053 - val_acc: 0.8889
Epoch 2/10
50/86 [================>.............] - ETA: 0s - loss: 0.5553 - acc: 0.780086/86 [==============================] - 0s - loss: 0.4171 - acc: 0.8488 - val_loss: 0.2215 - val_acc: 0.8889
Epoch 3/10
50/86 [================>.............] - ETA: 0s - loss: 0.2526 - acc: 0.900086/86 [==============================] - 0s - loss: 0.2251 - acc: 0.9302 - val_loss: 0.1185 - val_acc: 1.0000
Epoch 4/10
50/86 [================>.............] - ETA: 0s - loss: 0.1093 - acc: 0.940086/86 [==============================] - 0s - loss: 0.1424 - acc: 0.9186 - val_loss: 0.1750 - val_acc: 0.8889
Epoch 5/10
50/86 [================>.............] - ETA: 0s - loss: 0.1686 - acc: 0.940086/86 [==============================] - 0s - loss: 0.1172 - acc: 0.9651 - val_loss: 0.0568 - val_acc: 1.0000
Epoch 6/10
50/86 [================>.............] - ETA: 0s - loss: 0.0288 - acc: 1.000086/86 [==============================] - 0s - loss: 0.0949 - acc: 0.9651 - val_loss: 0.0456 - val_acc: 1.0000
Epoch 7/10
50/86 [================>.............] - ETA: 0s - loss: 0.0215 - acc: 0.980086/86 [==============================] - 0s - loss: 0.0258 - acc: 0.9884 - val_loss: 0.0907 - val_acc: 0.8889
Epoch 8/10
50/86 [================>.............] - ETA: 0s - loss: 0.0092 - acc: 1.000086/86 [==============================] - 0s - loss: 0.0149 - acc: 1.0000 - val_loss: 0.2292 - val_acc: 0.8889
Epoch 9/10
50/86 [================>.............] - ETA: 0s - loss: 0.0037 - acc: 1.000086/86 [==============================] - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 0.3215 - val_acc: 0.8889
Epoch 10/10
50/86 [================>.............] - ETA: 0s - loss: 0.0425 - acc: 0.980086/86 [==============================] - 0s - loss: 0.0254 - acc: 0.9884 - val_loss: 0.1719 - val_acc: 0.8889
9.0
8
0.888888888889
[[8 0]
 [1 0]]
> training sub-category <entit>y ..
Using TensorFlow backend.
layer_2_cnn_word2vec.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:137: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3, max_pool_4], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_2_cnn_word2vec.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_2_cnn_word2vec.py:148: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
2018-09-24 22:05:39.245973: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:05:39.246010: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:05:39.246018: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:05:39.246034: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:05:39.246041: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:05:39.366673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-24 22:05:39.367161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2018-09-24 22:05:39.367181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-09-24 22:05:39.367189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-09-24 22:05:39.367200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
(40, 9600) (40,)
(217, 9600) (217,)
(16, 9600) (16,)
(41, 9600) (41,)
(38, 9600) (38,)
(103, 9600) (103,)
(13, 9600) (13,)
(10, 9600) (10,)
(42, 9600) (42,)
(93, 9600) (93,)
(56, 9600) (56,)
(112, 9600) (112,)
(103, 9600) (103,)
(16, 9600) (16,)
(27, 9600) (27,)
(62, 9600) (62,)
(4, 9600) (4,)
(10, 9600) (10,)
(12, 9600) (12,)
(2, 9600) (2,)
(15, 9600) (15,)
(1, 9600) (1,)
(2, 9600) (2,)
(5, 9600) (5,)
(1, 9600) (1,)
(4, 9600) (4,)
(7, 9600) (7,)
(2, 9600) (2,)
(16, 9600) (16,)
(4, 9600) (4,)
(2, 9600) (2,)
(4, 9600) (4,)
(1, 9600) (1,)
(6, 9600) (6,)
(94, 9600)
Train on 993 samples, validate on 94 samples
Epoch 1/10
 50/993 [>.............................] - ETA: 18s - loss: 2.8356 - acc: 0.0600150/993 [===>..........................] - ETA: 5s - loss: 2.7068 - acc: 0.1533 250/993 [======>.......................] - ETA: 3s - loss: 2.6838 - acc: 0.1840350/993 [=========>....................] - ETA: 2s - loss: 2.6578 - acc: 0.2000450/993 [============>.................] - ETA: 1s - loss: 2.6362 - acc: 0.2022550/993 [===============>..............] - ETA: 1s - loss: 2.5811 - acc: 0.2218650/993 [==================>...........] - ETA: 0s - loss: 2.5289 - acc: 0.2338750/993 [=====================>........] - ETA: 0s - loss: 2.4870 - acc: 0.2533850/993 [========================>.....] - ETA: 0s - loss: 2.4456 - acc: 0.2576950/993 [===========================>..] - ETA: 0s - loss: 2.3866 - acc: 0.2789993/993 [==============================] - 1s - loss: 2.3589 - acc: 0.2850 - val_loss: 2.1436 - val_acc: 0.3298
Epoch 2/10
 50/993 [>.............................] - ETA: 0s - loss: 1.8044 - acc: 0.4000200/993 [=====>........................] - ETA: 0s - loss: 1.6594 - acc: 0.5000350/993 [=========>....................] - ETA: 0s - loss: 1.6129 - acc: 0.5086500/993 [==============>...............] - ETA: 0s - loss: 1.6024 - acc: 0.5160650/993 [==================>...........] - ETA: 0s - loss: 1.5229 - acc: 0.5508800/993 [=======================>......] - ETA: 0s - loss: 1.4629 - acc: 0.5737950/993 [===========================>..] - ETA: 0s - loss: 1.4267 - acc: 0.5821993/993 [==============================] - 0s - loss: 1.4014 - acc: 0.5881 - val_loss: 1.4799 - val_acc: 0.5319
Epoch 3/10
 50/993 [>.............................] - ETA: 0s - loss: 0.9546 - acc: 0.7400200/993 [=====>........................] - ETA: 0s - loss: 0.8740 - acc: 0.7600350/993 [=========>....................] - ETA: 0s - loss: 0.8166 - acc: 0.7657500/993 [==============>...............] - ETA: 0s - loss: 0.8071 - acc: 0.7620650/993 [==================>...........] - ETA: 0s - loss: 0.7785 - acc: 0.7738800/993 [=======================>......] - ETA: 0s - loss: 0.7459 - acc: 0.7862950/993 [===========================>..] - ETA: 0s - loss: 0.7243 - acc: 0.7926993/993 [==============================] - 0s - loss: 0.7128 - acc: 0.7946 - val_loss: 1.1304 - val_acc: 0.6383
Epoch 4/10
 50/993 [>.............................] - ETA: 0s - loss: 0.3366 - acc: 0.9200200/993 [=====>........................] - ETA: 0s - loss: 0.3050 - acc: 0.9400350/993 [=========>....................] - ETA: 0s - loss: 0.2875 - acc: 0.9429500/993 [==============>...............] - ETA: 0s - loss: 0.2697 - acc: 0.9500650/993 [==================>...........] - ETA: 0s - loss: 0.2791 - acc: 0.9431800/993 [=======================>......] - ETA: 0s - loss: 0.2876 - acc: 0.9325950/993 [===========================>..] - ETA: 0s - loss: 0.2794 - acc: 0.9358993/993 [==============================] - 0s - loss: 0.2756 - acc: 0.9355 - val_loss: 0.9001 - val_acc: 0.7234
Epoch 5/10
 50/993 [>.............................] - ETA: 0s - loss: 0.1302 - acc: 0.9600200/993 [=====>........................] - ETA: 0s - loss: 0.1165 - acc: 0.9850350/993 [=========>....................] - ETA: 0s - loss: 0.1003 - acc: 0.9914500/993 [==============>...............] - ETA: 0s - loss: 0.1129 - acc: 0.9860650/993 [==================>...........] - ETA: 0s - loss: 0.1193 - acc: 0.9862800/993 [=======================>......] - ETA: 0s - loss: 0.1178 - acc: 0.9862950/993 [===========================>..] - ETA: 0s - loss: 0.1151 - acc: 0.9863993/993 [==============================] - 0s - loss: 0.1132 - acc: 0.9869 - val_loss: 0.7639 - val_acc: 0.7872
Epoch 6/10
 50/993 [>.............................] - ETA: 0s - loss: 0.0393 - acc: 1.0000200/993 [=====>........................] - ETA: 0s - loss: 0.0610 - acc: 0.9950350/993 [=========>....................] - ETA: 0s - loss: 0.0593 - acc: 0.9943500/993 [==============>...............] - ETA: 0s - loss: 0.0517 - acc: 0.9960650/993 [==================>...........] - ETA: 0s - loss: 0.0485 - acc: 0.9954800/993 [=======================>......] - ETA: 0s - loss: 0.0465 - acc: 0.9962950/993 [===========================>..] - ETA: 0s - loss: 0.0430 - acc: 0.9968993/993 [==============================] - 0s - loss: 0.0422 - acc: 0.9970 - val_loss: 0.8333 - val_acc: 0.7979
Epoch 7/10
 50/993 [>.............................] - ETA: 0s - loss: 0.0174 - acc: 1.0000200/993 [=====>........................] - ETA: 0s - loss: 0.0245 - acc: 1.0000350/993 [=========>....................] - ETA: 0s - loss: 0.0245 - acc: 0.9971500/993 [==============>...............] - ETA: 0s - loss: 0.0246 - acc: 0.9980650/993 [==================>...........] - ETA: 0s - loss: 0.0238 - acc: 0.9969800/993 [=======================>......] - ETA: 0s - loss: 0.0239 - acc: 0.9963950/993 [===========================>..] - ETA: 0s - loss: 0.0236 - acc: 0.9958993/993 [==============================] - 0s - loss: 0.0233 - acc: 0.9960 - val_loss: 0.7985 - val_acc: 0.7447
Epoch 8/10
 50/993 [>.............................] - ETA: 0s - loss: 0.0177 - acc: 1.0000200/993 [=====>........................] - ETA: 0s - loss: 0.0139 - acc: 1.0000350/993 [=========>....................] - ETA: 0s - loss: 0.0139 - acc: 1.0000500/993 [==============>...............] - ETA: 0s - loss: 0.0133 - acc: 1.0000650/993 [==================>...........] - ETA: 0s - loss: 0.0135 - acc: 1.0000800/993 [=======================>......] - ETA: 0s - loss: 0.0156 - acc: 0.9988950/993 [===========================>..] - ETA: 0s - loss: 0.0189 - acc: 0.9979993/993 [==============================] - 0s - loss: 0.0184 - acc: 0.9980 - val_loss: 0.9673 - val_acc: 0.7660
Epoch 9/10
 50/993 [>.............................] - ETA: 0s - loss: 0.0072 - acc: 1.0000200/993 [=====>........................] - ETA: 0s - loss: 0.0119 - acc: 1.0000350/993 [=========>....................] - ETA: 0s - loss: 0.0126 - acc: 1.0000500/993 [==============>...............] - ETA: 0s - loss: 0.0133 - acc: 1.0000650/993 [==================>...........] - ETA: 0s - loss: 0.0140 - acc: 0.9985800/993 [=======================>......] - ETA: 0s - loss: 0.0128 - acc: 0.9988950/993 [===========================>..] - ETA: 0s - loss: 0.0125 - acc: 0.9989993/993 [==============================] - 0s - loss: 0.0159 - acc: 0.9980 - val_loss: 0.8641 - val_acc: 0.7872
Epoch 10/10
 50/993 [>.............................] - ETA: 0s - loss: 0.0060 - acc: 1.0000200/993 [=====>........................] - ETA: 0s - loss: 0.0087 - acc: 1.0000350/993 [=========>....................] - ETA: 0s - loss: 0.0093 - acc: 1.0000500/993 [==============>...............] - ETA: 0s - loss: 0.0094 - acc: 1.0000650/993 [==================>...........] - ETA: 0s - loss: 0.0092 - acc: 1.0000800/993 [=======================>......] - ETA: 0s - loss: 0.0085 - acc: 1.0000950/993 [===========================>..] - ETA: 0s - loss: 0.0086 - acc: 1.0000993/993 [==============================] - 0s - loss: 0.0084 - acc: 1.0000 - val_loss: 0.8162 - val_acc: 0.7660
94.0
72
0.765957446809
[[10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  9  0  1  0  0  0  0  0  0  1  0  0  0  0  1  0]
 [ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 1  7  0  6  0  1  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  0  0  1  0  0  0  1  0  0]
 [ 0  0  0  0  0  0  0  0  0  6  0  1  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  1  0 14  1  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]
 [ 0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  4]]
> training sub-category <description> ..
Using TensorFlow backend.
layer_2_cnn_word2vec.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:137: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3, max_pool_4], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_2_cnn_word2vec.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_2_cnn_word2vec.py:148: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
2018-09-24 22:06:36.017088: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:06:36.017121: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:06:36.017128: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:06:36.017134: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:06:36.017139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:06:36.136432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-24 22:06:36.136936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2018-09-24 22:06:36.136957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-09-24 22:06:36.136964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-09-24 22:06:36.136974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
(421, 9600) (421,)
(276, 9600) (276,)
(274, 9600) (274,)
(191, 9600) (191,)
(123, 9600) (123,)
(2, 9600) (2,)
(7, 9600) (7,)
(6, 9600) (6,)
(138, 9600)
Train on 1162 samples, validate on 138 samples
Epoch 1/10
  50/1162 [>.............................] - ETA: 21s - loss: 1.3801 - acc: 0.2400 150/1162 [==>...........................] - ETA: 7s - loss: 1.3260 - acc: 0.3533  250/1162 [=====>........................] - ETA: 4s - loss: 1.2485 - acc: 0.4240 350/1162 [========>.....................] - ETA: 2s - loss: 1.1203 - acc: 0.4971 450/1162 [==========>...................] - ETA: 1s - loss: 1.0428 - acc: 0.5489 550/1162 [=============>................] - ETA: 1s - loss: 0.9834 - acc: 0.5818 650/1162 [===============>..............] - ETA: 1s - loss: 0.9167 - acc: 0.6154 750/1162 [==================>...........] - ETA: 0s - loss: 0.8698 - acc: 0.6347 850/1162 [====================>.........] - ETA: 0s - loss: 0.8060 - acc: 0.66351000/1162 [========================>.....] - ETA: 0s - loss: 0.7598 - acc: 0.68901150/1162 [============================>.] - ETA: 0s - loss: 0.7261 - acc: 0.70871162/1162 [==============================] - 1s - loss: 0.7249 - acc: 0.7083 - val_loss: 0.0580 - val_acc: 0.9783
Epoch 2/10
  50/1162 [>.............................] - ETA: 0s - loss: 0.4179 - acc: 0.9200 200/1162 [====>.........................] - ETA: 0s - loss: 0.2874 - acc: 0.9100 350/1162 [========>.....................] - ETA: 0s - loss: 0.2996 - acc: 0.9029 500/1162 [===========>..................] - ETA: 0s - loss: 0.2592 - acc: 0.9120 650/1162 [===============>..............] - ETA: 0s - loss: 0.2681 - acc: 0.9077 800/1162 [===================>..........] - ETA: 0s - loss: 0.2492 - acc: 0.9150 950/1162 [=======================>......] - ETA: 0s - loss: 0.2411 - acc: 0.91581100/1162 [===========================>..] - ETA: 0s - loss: 0.2356 - acc: 0.91641162/1162 [==============================] - 0s - loss: 0.2325 - acc: 0.9182 - val_loss: 0.0160 - val_acc: 1.0000
Epoch 3/10
  50/1162 [>.............................] - ETA: 0s - loss: 0.0764 - acc: 0.9800 200/1162 [====>.........................] - ETA: 0s - loss: 0.0610 - acc: 0.9800 350/1162 [========>.....................] - ETA: 0s - loss: 0.0659 - acc: 0.9771 500/1162 [===========>..................] - ETA: 0s - loss: 0.0581 - acc: 0.9800 650/1162 [===============>..............] - ETA: 0s - loss: 0.0498 - acc: 0.9815 800/1162 [===================>..........] - ETA: 0s - loss: 0.0457 - acc: 0.9838 950/1162 [=======================>......] - ETA: 0s - loss: 0.0407 - acc: 0.98631100/1162 [===========================>..] - ETA: 0s - loss: 0.0531 - acc: 0.98361162/1162 [==============================] - 0s - loss: 0.0520 - acc: 0.9836 - val_loss: 0.0132 - val_acc: 0.9928
Epoch 4/10
  50/1162 [>.............................] - ETA: 0s - loss: 0.0093 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 0.0060 - acc: 1.0000 350/1162 [========>.....................] - ETA: 0s - loss: 0.0073 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 0.0086 - acc: 0.9980 650/1162 [===============>..............] - ETA: 0s - loss: 0.0081 - acc: 0.9985 800/1162 [===================>..........] - ETA: 0s - loss: 0.0122 - acc: 0.9975 950/1162 [=======================>......] - ETA: 0s - loss: 0.0116 - acc: 0.99791100/1162 [===========================>..] - ETA: 0s - loss: 0.0111 - acc: 0.99821162/1162 [==============================] - 0s - loss: 0.0120 - acc: 0.9974 - val_loss: 0.0200 - val_acc: 0.9928
Epoch 5/10
  50/1162 [>.............................] - ETA: 0s - loss: 0.0054 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 0.0031 - acc: 1.0000 350/1162 [========>.....................] - ETA: 0s - loss: 0.0031 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 0.0035 - acc: 1.0000 650/1162 [===============>..............] - ETA: 0s - loss: 0.0032 - acc: 1.0000 800/1162 [===================>..........] - ETA: 0s - loss: 0.0033 - acc: 1.0000 950/1162 [=======================>......] - ETA: 0s - loss: 0.0037 - acc: 1.00001100/1162 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 1.00001162/1162 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0104 - val_acc: 0.9928
Epoch 6/10
  50/1162 [>.............................] - ETA: 0s - loss: 0.0021 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 0.0024 - acc: 1.0000 350/1162 [========>.....................] - ETA: 0s - loss: 0.0020 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 0.0022 - acc: 1.0000 650/1162 [===============>..............] - ETA: 0s - loss: 0.0023 - acc: 1.0000 800/1162 [===================>..........] - ETA: 0s - loss: 0.0022 - acc: 1.0000 950/1162 [=======================>......] - ETA: 0s - loss: 0.0020 - acc: 1.00001100/1162 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 1.00001162/1162 [==============================] - 0s - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0223 - val_acc: 0.9855
Epoch 7/10
  50/1162 [>.............................] - ETA: 0s - loss: 0.0012 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 9.0144e-04 - acc: 1.0000 350/1162 [========>.....................] - ETA: 0s - loss: 9.7108e-04 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 0.0013 - acc: 1.0000     650/1162 [===============>..............] - ETA: 0s - loss: 0.0012 - acc: 1.0000 800/1162 [===================>..........] - ETA: 0s - loss: 0.0011 - acc: 1.0000 950/1162 [=======================>......] - ETA: 0s - loss: 0.0011 - acc: 1.00001100/1162 [===========================>..] - ETA: 0s - loss: 0.0010 - acc: 1.00001162/1162 [==============================] - 0s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0180 - val_acc: 0.9855
Epoch 8/10
  50/1162 [>.............................] - ETA: 0s - loss: 3.9279e-04 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 6.0534e-04 - acc: 1.0000 350/1162 [========>.....................] - ETA: 0s - loss: 5.8067e-04 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 5.9311e-04 - acc: 1.0000 650/1162 [===============>..............] - ETA: 0s - loss: 5.8452e-04 - acc: 1.0000 800/1162 [===================>..........] - ETA: 0s - loss: 5.7165e-04 - acc: 1.0000 950/1162 [=======================>......] - ETA: 0s - loss: 5.6861e-04 - acc: 1.00001100/1162 [===========================>..] - ETA: 0s - loss: 5.7730e-04 - acc: 1.00001162/1162 [==============================] - 0s - loss: 6.1473e-04 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9928
Epoch 9/10
  50/1162 [>.............................] - ETA: 0s - loss: 9.0107e-04 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 0.0011 - acc: 1.0000     350/1162 [========>.....................] - ETA: 0s - loss: 8.7513e-04 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 7.5351e-04 - acc: 1.0000 650/1162 [===============>..............] - ETA: 0s - loss: 7.2305e-04 - acc: 1.0000 800/1162 [===================>..........] - ETA: 0s - loss: 6.6160e-04 - acc: 1.0000 950/1162 [=======================>......] - ETA: 0s - loss: 6.5943e-04 - acc: 1.00001100/1162 [===========================>..] - ETA: 0s - loss: 6.0676e-04 - acc: 1.00001162/1162 [==============================] - 0s - loss: 5.8702e-04 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 0.9928
Epoch 10/10
  50/1162 [>.............................] - ETA: 0s - loss: 2.9352e-04 - acc: 1.0000 200/1162 [====>.........................] - ETA: 0s - loss: 7.0985e-04 - acc: 1.0000 350/1162 [========>.....................] - ETA: 0s - loss: 6.1178e-04 - acc: 1.0000 500/1162 [===========>..................] - ETA: 0s - loss: 7.7019e-04 - acc: 1.0000 650/1162 [===============>..............] - ETA: 0s - loss: 6.9131e-04 - acc: 1.0000 800/1162 [===================>..........] - ETA: 0s - loss: 6.0796e-04 - acc: 1.0000 950/1162 [=======================>......] - ETA: 0s - loss: 6.7227e-04 - acc: 1.00001100/1162 [===========================>..] - ETA: 0s - loss: 6.2568e-04 - acc: 1.00001162/1162 [==============================] - 0s - loss: 6.0879e-04 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 0.9928
138.0
137
0.992753623188
[[123   0   0   0]
 [  0   2   0   0]
 [  1   0   6   0]
 [  0   0   0   6]]
> training sub-category <human> ..
Using TensorFlow backend.
layer_2_cnn_word2vec.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:137: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3, max_pool_4], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_2_cnn_word2vec.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_2_cnn_word2vec.py:148: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
2018-09-24 22:07:33.724709: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:07:33.724752: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:07:33.724771: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:07:33.724777: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:07:33.724783: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:07:33.844844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-24 22:07:33.845359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2018-09-24 22:07:33.845380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-09-24 22:07:33.845388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-09-24 22:07:33.845399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
(47, 9600) (47,)
(189, 9600) (189,)
(3, 9600) (3,)
(6, 9600) (6,)
(9, 9600)
Train on 236 samples, validate on 9 samples
Epoch 1/10
 50/236 [=====>........................] - ETA: 3s - loss: 0.6866 - acc: 0.5600150/236 [==================>...........] - ETA: 0s - loss: 0.4639 - acc: 0.7200236/236 [==============================] - 1s - loss: 0.3525 - acc: 0.7881 - val_loss: 0.0574 - val_acc: 1.0000
Epoch 2/10
 50/236 [=====>........................] - ETA: 0s - loss: 0.0454 - acc: 1.0000150/236 [==================>...........] - ETA: 0s - loss: 0.0557 - acc: 0.9933236/236 [==============================] - 0s - loss: 0.0463 - acc: 0.9915 - val_loss: 0.0250 - val_acc: 1.0000
Epoch 3/10
 50/236 [=====>........................] - ETA: 0s - loss: 0.0288 - acc: 0.9800150/236 [==================>...........] - ETA: 0s - loss: 0.0474 - acc: 0.9867236/236 [==============================] - 0s - loss: 0.0302 - acc: 0.9915 - val_loss: 0.0018 - val_acc: 1.0000
Epoch 4/10
 50/236 [=====>........................] - ETA: 0s - loss: 0.0410 - acc: 0.9800200/236 [========================>.....] - ETA: 0s - loss: 0.0111 - acc: 0.9950236/236 [==============================] - 0s - loss: 0.0094 - acc: 0.9958 - val_loss: 0.0564 - val_acc: 1.0000
Epoch 5/10
 50/236 [=====>........................] - ETA: 0s - loss: 0.0153 - acc: 0.9800200/236 [========================>.....] - ETA: 0s - loss: 0.0040 - acc: 0.9950236/236 [==============================] - 0s - loss: 0.0034 - acc: 0.9958 - val_loss: 0.5144 - val_acc: 0.8889
Epoch 6/10
 50/236 [=====>........................] - ETA: 0s - loss: 0.0368 - acc: 0.9800200/236 [========================>.....] - ETA: 0s - loss: 0.0094 - acc: 0.9950236/236 [==============================] - 0s - loss: 0.0080 - acc: 0.9958 - val_loss: 0.1731 - val_acc: 0.8889
Epoch 7/10
 50/236 [=====>........................] - ETA: 0s - loss: 8.0231e-05 - acc: 1.0000200/236 [========================>.....] - ETA: 0s - loss: 7.2523e-05 - acc: 1.0000236/236 [==============================] - 0s - loss: 6.1885e-05 - acc: 1.0000 - val_loss: 0.0244 - val_acc: 1.0000
Epoch 8/10
 50/236 [=====>........................] - ETA: 0s - loss: 3.0887e-06 - acc: 1.0000200/236 [========================>.....] - ETA: 0s - loss: 1.3499e-05 - acc: 1.0000236/236 [==============================] - 0s - loss: 2.1179e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000
Epoch 9/10
 50/236 [=====>........................] - ETA: 0s - loss: 4.1831e-06 - acc: 1.0000200/236 [========================>.....] - ETA: 0s - loss: 4.6714e-05 - acc: 1.0000236/236 [==============================] - 0s - loss: 4.6390e-05 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000
Epoch 10/10
 50/236 [=====>........................] - ETA: 0s - loss: 7.9421e-06 - acc: 1.0000200/236 [========================>.....] - ETA: 0s - loss: 4.6203e-05 - acc: 1.0000236/236 [==============================] - 0s - loss: 5.1210e-05 - acc: 1.0000 - val_loss: 7.4560e-04 - val_acc: 1.0000
9.0
9
1.0
[[3 0]
 [0 6]]
> training sub-category <location> ..
Using TensorFlow backend.
layer_2_cnn_word2vec.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:137: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3, max_pool_4], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_2_cnn_word2vec.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_2_cnn_word2vec.py:148: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
(129, 9600) (129,)
(18, 9600) (18,)
(18, 9600)
Traceback (most recent call last):
  File "layer_2_cnn_word2vec.py", line 148, in <module>
    model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
  File "build/bdist.linux-x86_64/egg/keras/engine/training.py", line 1522, in fit
  File "build/bdist.linux-x86_64/egg/keras/engine/training.py", line 1393, in _standardize_user_data
  File "build/bdist.linux-x86_64/egg/keras/engine/training.py", line 277, in _check_loss_and_target_compatibility
ValueError: You are passing a target array of shape (129, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:
```
from keras.utils.np_utils import to_categorical
y_binary = to_categorical(y_int)
```

Alternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.
> training sub-category <numeric> ..
Using TensorFlow backend.
layer_2_cnn_word2vec.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (2, 300), activation="relu")`
  conv_1 = Conv2D(500, 2, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (3, 300), activation="relu")`
  conv_2 = Conv2D(500, 3, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (4, 300), activation="relu")`
  conv_3 = Conv2D(500, 4, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(500, (5, 300), activation="relu")`
  conv_4 = Conv2D(500, 5, 300, activation= 'relu')(inputs)
layer_2_cnn_word2vec.py:137: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged = merge([max_pool_1, max_pool_2, max_pool_3, max_pool_4], mode= 'concat')
/usr/local/lib/python2.7/dist-packages/Keras-2.0.8-py2.7.egg/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
layer_2_cnn_word2vec.py:145: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=Tensor("in...)`
  model = Model(input= inputs, output= output)
layer_2_cnn_word2vec.py:148: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model.fit(x_train, y_train, batch_size= 50, nb_epoch= int(epochs) , validation_data =(x_test, y_test))
2018-09-24 22:09:18.670845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:09:18.670883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:09:18.670891: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:09:18.670897: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:09:18.670903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-09-24 22:09:18.790808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-24 22:09:18.791290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2018-09-24 22:09:18.791320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-09-24 22:09:18.791328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-09-24 22:09:18.791339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
(52, 9600) (52,)
(71, 9600) (71,)
(11, 9600) (11,)
(34, 9600) (34,)
(8, 9600) (8,)
(75, 9600) (75,)
(363, 9600) (363,)
(218, 9600) (218,)
(9, 9600) (9,)
(27, 9600) (27,)
(12, 9600) (12,)
(3, 9600) (3,)
(4, 9600) (4,)
(16, 9600) (16,)
(5, 9600) (5,)
(8, 9600) (8,)
(9, 9600) (9,)
(47, 9600) (47,)
(6, 9600) (6,)
(3, 9600) (3,)
(113, 9600)
Train on 868 samples, validate on 113 samples
Epoch 1/10
 50/868 [>.............................] - ETA: 16s - loss: 2.2954 - acc: 0.1600150/868 [====>.........................] - ETA: 5s - loss: 1.9622 - acc: 0.3867 250/868 [=======>......................] - ETA: 2s - loss: 1.8790 - acc: 0.4320350/868 [===========>..................] - ETA: 1s - loss: 1.7172 - acc: 0.4886450/868 [==============>...............] - ETA: 1s - loss: 1.6202 - acc: 0.5133550/868 [==================>...........] - ETA: 0s - loss: 1.5392 - acc: 0.5345650/868 [=====================>........] - ETA: 0s - loss: 1.4372 - acc: 0.5677750/868 [========================>.....] - ETA: 0s - loss: 1.3651 - acc: 0.5880868/868 [==============================] - 1s - loss: 1.2783 - acc: 0.6106 - val_loss: 1.0942 - val_acc: 0.6106
Epoch 2/10
 50/868 [>.............................] - ETA: 0s - loss: 0.6751 - acc: 0.8000200/868 [=====>........................] - ETA: 0s - loss: 0.5814 - acc: 0.8000350/868 [===========>..................] - ETA: 0s - loss: 0.5524 - acc: 0.8143500/868 [================>.............] - ETA: 0s - loss: 0.4922 - acc: 0.8420650/868 [=====================>........] - ETA: 0s - loss: 0.4700 - acc: 0.8492800/868 [==========================>...] - ETA: 0s - loss: 0.4596 - acc: 0.8587868/868 [==============================] - 0s - loss: 0.4534 - acc: 0.8594 - val_loss: 0.7558 - val_acc: 0.7345
Epoch 3/10
 50/868 [>.............................] - ETA: 0s - loss: 0.2614 - acc: 0.9400200/868 [=====>........................] - ETA: 0s - loss: 0.1699 - acc: 0.9700350/868 [===========>..................] - ETA: 0s - loss: 0.1863 - acc: 0.9543500/868 [================>.............] - ETA: 0s - loss: 0.1911 - acc: 0.9480650/868 [=====================>........] - ETA: 0s - loss: 0.1772 - acc: 0.9523800/868 [==========================>...] - ETA: 0s - loss: 0.1748 - acc: 0.9538868/868 [==============================] - 0s - loss: 0.1841 - acc: 0.9505 - val_loss: 0.4968 - val_acc: 0.8319
Epoch 4/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0750 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0686 - acc: 0.9950350/868 [===========>..................] - ETA: 0s - loss: 0.0624 - acc: 0.9914500/868 [================>.............] - ETA: 0s - loss: 0.0687 - acc: 0.9880650/868 [=====================>........] - ETA: 0s - loss: 0.0646 - acc: 0.9892800/868 [==========================>...] - ETA: 0s - loss: 0.0760 - acc: 0.9862868/868 [==============================] - 0s - loss: 0.0753 - acc: 0.9862 - val_loss: 0.3548 - val_acc: 0.9027
Epoch 5/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0151 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0249 - acc: 1.0000350/868 [===========>..................] - ETA: 0s - loss: 0.0256 - acc: 1.0000500/868 [================>.............] - ETA: 0s - loss: 0.0267 - acc: 0.9980650/868 [=====================>........] - ETA: 0s - loss: 0.0251 - acc: 0.9985800/868 [==========================>...] - ETA: 0s - loss: 0.0242 - acc: 0.9975868/868 [==============================] - 0s - loss: 0.0318 - acc: 0.9965 - val_loss: 0.3730 - val_acc: 0.8938
Epoch 6/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0059 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0097 - acc: 1.0000350/868 [===========>..................] - ETA: 0s - loss: 0.0155 - acc: 1.0000500/868 [================>.............] - ETA: 0s - loss: 0.0135 - acc: 1.0000650/868 [=====================>........] - ETA: 0s - loss: 0.0130 - acc: 1.0000800/868 [==========================>...] - ETA: 0s - loss: 0.0136 - acc: 1.0000868/868 [==============================] - 0s - loss: 0.0199 - acc: 0.9988 - val_loss: 0.3385 - val_acc: 0.8938
Epoch 7/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0040 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0144 - acc: 0.9950350/868 [===========>..................] - ETA: 0s - loss: 0.0118 - acc: 0.9971500/868 [================>.............] - ETA: 0s - loss: 0.0105 - acc: 0.9980650/868 [=====================>........] - ETA: 0s - loss: 0.0091 - acc: 0.9985800/868 [==========================>...] - ETA: 0s - loss: 0.0084 - acc: 0.9988868/868 [==============================] - 0s - loss: 0.0081 - acc: 0.9988 - val_loss: 0.3436 - val_acc: 0.8761
Epoch 8/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0049 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0047 - acc: 1.0000350/868 [===========>..................] - ETA: 0s - loss: 0.0053 - acc: 1.0000500/868 [================>.............] - ETA: 0s - loss: 0.0322 - acc: 0.9980650/868 [=====================>........] - ETA: 0s - loss: 0.0266 - acc: 0.9985800/868 [==========================>...] - ETA: 0s - loss: 0.0223 - acc: 0.9988868/868 [==============================] - 0s - loss: 0.0207 - acc: 0.9988 - val_loss: 0.3455 - val_acc: 0.9204
Epoch 9/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0040 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0034 - acc: 1.0000350/868 [===========>..................] - ETA: 0s - loss: 0.0038 - acc: 1.0000500/868 [================>.............] - ETA: 0s - loss: 0.0143 - acc: 0.9960650/868 [=====================>........] - ETA: 0s - loss: 0.0115 - acc: 0.9969800/868 [==========================>...] - ETA: 0s - loss: 0.0099 - acc: 0.9975868/868 [==============================] - 0s - loss: 0.0093 - acc: 0.9977 - val_loss: 0.3230 - val_acc: 0.9027
Epoch 10/10
 50/868 [>.............................] - ETA: 0s - loss: 0.0039 - acc: 1.0000200/868 [=====>........................] - ETA: 0s - loss: 0.0033 - acc: 1.0000350/868 [===========>..................] - ETA: 0s - loss: 0.0120 - acc: 0.9971500/868 [================>.............] - ETA: 0s - loss: 0.0164 - acc: 0.9960650/868 [=====================>........] - ETA: 0s - loss: 0.0149 - acc: 0.9954800/868 [==========================>...] - ETA: 0s - loss: 0.0126 - acc: 0.9962868/868 [==============================] - 0s - loss: 0.0119 - acc: 0.9965 - val_loss: 0.4199 - val_acc: 0.8938
113.0
101
0.893805309735
[[ 6  0  0  0  2  2  0  1  0  1]
 [ 0  3  0  0  0  0  0  0  0  0]
 [ 0  0  4  0  0  0  0  0  0  0]
 [ 2  0  0 11  1  1  0  0  1  0]
 [ 0  0  0  0  4  1  0  0  0  0]
 [ 0  0  0  0  0  8  0  0  0  0]
 [ 0  0  0  0  0  0  9  0  0  0]
 [ 0  0  0  0  0  0  0 47  0  0]
 [ 0  0  0  0  0  0  0  0  6  0]
 [ 0  0  0  0  0  0  0  0  0  3]]
